{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -U \"transformers>=4.44.0\" accelerate huggingface_hub\n",
        "!pip install -U sentencepiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGrzL1CLjtY8",
        "outputId": "e2359ada-9018-4006-e2d0-4280c910c71e"
      },
      "id": "zGrzL1CLjtY8",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers>=4.44.0 in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (0.36.0)\n",
            "Collecting huggingface_hub\n",
            "  Using cached huggingface_hub-1.2.1-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers>=4.44.0) (3.20.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.44.0) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.44.0) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.44.0) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.44.0) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers>=4.44.0) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.44.0) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.44.0) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.44.0) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.9.0+cu126)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (1.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.5.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.44.0) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.44.0) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.44.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.44.0) (2025.11.12)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (0.2.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1da1f260",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1da1f260",
        "outputId": "9e6898d7-7d56-493e-f28b-5e6805127830"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading descriptions dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Descriptions head:\n",
            "                                            Position  \\\n",
            "0      10 + Blockchain Nodes / Masternodes to set up   \n",
            "1       10 .NET Developers (Middle and Senior level)   \n",
            "2  10X Engineer (co-founder, #4 employee, USD 11-...   \n",
            "3                          16 - Amazon Brand Manager   \n",
            "4                          16 - Amazon Brand Manager   \n",
            "\n",
            "                                    Long Description    Company Name  \\\n",
            "0  *Requirements*\\r\\n\\r\\nWe're looking for a long...    MyCointainer   \n",
            "1  Greetings! My name is Maria, I am in urgent ne...  TechScout.tech   \n",
            "2  **Product**\\r\\nThe product is a live video cha...        Innoteka   \n",
            "3  Currently, TCM expanding its activities to Ukr...       FirstFive   \n",
            "4  Hello,\\r\\nWe, MIMIRB2B, are an outstaff compan...        MimirB2B   \n",
            "\n",
            "  Exp Years Primary Keyword English Level                  Published  \\\n",
            "0        2y        Sysadmin  intermediate  2020-10-01T00:00:00+03:00   \n",
            "1        2y            .NET  intermediate  2022-03-01T00:00:00+02:00   \n",
            "2        5y      JavaScript        fluent  2021-07-01T00:00:00+03:00   \n",
            "3        2y       Marketing         upper  2022-01-01T00:00:00+02:00   \n",
            "4        1y       Marketing         upper  2021-12-01T00:00:00+02:00   \n",
            "\n",
            "  Long Description_lang                                    id  \\\n",
            "0                    en  c0ca96e7-85df-50df-a64e-d934cd02a170   \n",
            "1                    en  64f4b7ea-36e4-5bdd-a8b1-185f32f7dc7f   \n",
            "2                    en  b9a1303e-dd0c-5ed1-8f62-be2bc4c7da4f   \n",
            "3                    en  99cb3f4a-9b4b-53d9-9a3b-bab2c22da346   \n",
            "4                    en  bc1419f7-28e2-582b-8d53-22e28b2f0210   \n",
            "\n",
            "   __index_level_0__  \n",
            "0              27461  \n",
            "1              27462  \n",
            "2              27463  \n",
            "3              27464  \n",
            "4              27465  \n",
            "------------------------------\n",
            "Loading profiles dataset...\n",
            "Profiles head:\n",
            "                                            Position  \\\n",
            "0  13 years of exp || Solidity, C#, JavaScript ||...   \n",
            "1                                       1c Developer   \n",
            "2                                       1C developer   \n",
            "3                                       1C Developer   \n",
            "4                     #1 Customer Support Specialist   \n",
            "\n",
            "                                            Moreinfo  \\\n",
            "0  Who am I:\\r\\n- 13 years of commercial experien...   \n",
            "1  Worked on a mobile application for tracking trips   \n",
            "2  1 am an 1C developer. I deployed an 1C to typo...   \n",
            "3  Perfect knowledge of  1C:Enterprise Platform.\\...   \n",
            "4  Tech addicted, experienced customer support wi...   \n",
            "\n",
            "                                         Looking For  \\\n",
            "0  I am interested in:\\r\\n- part-time engagement;...   \n",
            "1                                               None   \n",
            "2                                               None   \n",
            "3                                               None   \n",
            "4  In terms of the nature of the Customer Support...   \n",
            "\n",
            "                                          Highlights Primary Keyword  \\\n",
            "0  Landed a role of Director of Blockchain Develo...            Lead   \n",
            "1                                               None           Other   \n",
            "2                                               None         Flutter   \n",
            "3                                               None           Other   \n",
            "4  thousands of resolved problems, also, thousand...         Support   \n",
            "\n",
            "  English Level  Experience Years  \\\n",
            "0        fluent              11.0   \n",
            "1  intermediate               3.0   \n",
            "2  intermediate              11.0   \n",
            "3  intermediate              11.0   \n",
            "4        fluent               2.5   \n",
            "\n",
            "                                                  CV CV_lang  \\\n",
            "0  Landed a role of Director of Blockchain Develo...      en   \n",
            "1  \\nWorked on a mobile application for tracking ...      en   \n",
            "2  \\n1 am an 1C developer. I deployed an 1C to ty...      en   \n",
            "3  \\nPerfect knowledge of  1C:Enterprise Platform...      en   \n",
            "4  thousands of resolved problems, also, thousand...      en   \n",
            "\n",
            "                                     id  __index_level_0__  \n",
            "0  50534b61-6826-52b1-9ac5-bfd2cfa348ec              24230  \n",
            "1  c2b9ea56-b5c8-50ec-a63b-053a5c8ff467              24231  \n",
            "2  b3bfe3ed-ec25-56b2-8aaa-8c629120538e              24232  \n",
            "3  163fb9a3-3695-5dc3-b1d0-e0038ce4d0a5              24233  \n",
            "4  3f201183-db58-5333-9139-74c16059dc4a              24234  \n",
            "------------------------------\n",
            "Logging in to Hugging Face Hub...\n",
            "··········\n",
            "Login successful.\n",
            "------------------------------\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from getpass import getpass\n",
        "from huggingface_hub import login\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TextIteratorStreamer\n",
        "import torch\n",
        "import threading\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"Loading descriptions dataset...\")\n",
        "descriptions = pd.read_parquet(\"hf://datasets/lang-uk/recruitment-dataset-job-descriptions-english/data/train-00000-of-00001.parquet\")\n",
        "print(\"Descriptions head:\")\n",
        "print(descriptions.head())\n",
        "print(\"-\" * 30)\n",
        "\n",
        "print(\"Loading profiles dataset...\")\n",
        "profile = pd.read_parquet(\"hf://datasets/lang-uk/recruitment-dataset-candidate-profiles-english/data/train-00000-of-00001.parquet\")\n",
        "print(\"Profiles head:\")\n",
        "print(profile.head())\n",
        "print(\"-\" * 30)\n",
        "\n",
        "print(\"Logging in to Hugging Face Hub...\")\n",
        "try:\n",
        "    HF_TOKEN = getpass()\n",
        "    login(HF_TOKEN)\n",
        "    print(\"Login successful.\")\n",
        "except Exception as e:\n",
        "    print(f\"Login failed: {e}\")\n",
        "print(\"-\" * 30)\n",
        "MODELS_TO_TEST = {\n",
        "    # \"gemma-2-2b-it\": \"google/gemma-2-2b-it\",\n",
        "    # \"gemma-2-9b-it\": \"google/gemma-2-9b-it\",\n",
        "    # \"gemma-2-27b-it\": \"google/gemma-2-27b-it\",\n",
        "\n",
        "    # \"qwen2-0.5b-instruct\": \"Qwen/Qwen2-0.5B-Instruct\",\n",
        "    # \"qwen2-1.5b-instruct\": \"Qwen/Qwen2-1.5B-Instruct\",\n",
        "    # \"qwen2-7b-instruct\":   \"Qwen/Qwen2-7B-Instruct\",\n",
        "    # \"qwen2-72b-instruct\":  \"Qwen/Qwen2-72B-Instruct\",\n",
        "\n",
        "    # \"mistral-7b-instruct\": \"mistralai/Mistral-7B-Instruct-v0.3\",\n",
        "    # \"mistral-nemo-12b\":    \"mistralai/Mistral-Nemo-Instruct-2407\",\n",
        "    # \"mixtral-8x7b-instruct\": \"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
        "\n",
        "    # \"phi-3-mini-4k-instruct\":   \"microsoft/Phi-3-mini-4k-instruct\",\n",
        "    # \"phi-3-small-8k-instruct\":  \"microsoft/Phi-3-small-8k-instruct\",\n",
        "    \"phi-3-medium-4k-instruct\": \"microsoft/Phi-3-medium-4k-instruct\"\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "318d89b5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "318d89b5",
        "outputId": "0ba15f1e-be2b-416c-fae2-b65d046cdee2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using GPU: Tesla T4\n",
            "GPU memory: 14.7 GB\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/backends/__init__.py:46: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
            "  self.setter(val)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "if device == \"cuda\":\n",
        "    print(f\"Using GPU: {torch.cuda.get_device_name()}\")\n",
        "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "    torch.backends.cudnn.allow_tf32 = True\n",
        "else:\n",
        "    print(\"Using CPU - це буде повільно!\")\n",
        "\n",
        "print(\"-\" * 30)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ec402e9",
      "metadata": {
        "id": "6ec402e9"
      },
      "outputs": [],
      "source": [
        "\n",
        "def generate_resumes_for_model(model_name, model_path, prompts, device):\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Loading {model_name}...\")\n",
        "\n",
        "    try:\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_path,\n",
        "            device_map=\"auto\",\n",
        "            torch_dtype=torch.bfloat16,\n",
        "            trust_remote_code=True\n",
        "        ).eval()\n",
        "\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "\n",
        "        if tokenizer.pad_token is None:\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "        print(f\"Model {model_name} loaded successfully\")\n",
        "\n",
        "        generated_resumes = []\n",
        "        generation_times = []\n",
        "\n",
        "        test_prompts = prompts[:1]\n",
        "\n",
        "        for i, prompt_text in enumerate(tqdm(test_prompts, desc=f\"Generating with {model_name}\")):\n",
        "            start_time = time.time()\n",
        "\n",
        "            try:\n",
        "                messages = [{\"role\": \"user\", \"content\": prompt_text}]\n",
        "\n",
        "                if hasattr(tokenizer, 'apply_chat_template'):\n",
        "                    try:\n",
        "                        prompt_with_template = tokenizer.apply_chat_template(\n",
        "                            messages, tokenize=False, add_generation_prompt=True\n",
        "                        )\n",
        "                    except:\n",
        "                        prompt_with_template = prompt_text\n",
        "                else:\n",
        "                    prompt_with_template = prompt_text\n",
        "\n",
        "                inputs = tokenizer(prompt_with_template, return_tensors=\"pt\", truncation=True, max_length=2048)\n",
        "                inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    outputs = model.generate(\n",
        "                        **inputs,\n",
        "                        max_new_tokens=512,\n",
        "                        temperature=0.7,\n",
        "                        top_p=0.9,\n",
        "                        do_sample=True,\n",
        "                        eos_token_id=tokenizer.eos_token_id,\n",
        "                        pad_token_id=tokenizer.pad_token_id,\n",
        "                        repetition_penalty=1.1\n",
        "                    )\n",
        "\n",
        "                resume_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "                end_time = time.time()\n",
        "                generation_time = end_time - start_time\n",
        "\n",
        "                generated_resumes.append({\n",
        "                    \"candidate_id\": i,\n",
        "                    \"resume\": resume_text,\n",
        "                    \"generation_time_seconds\": generation_time,\n",
        "                    \"model\": model_name\n",
        "                })\n",
        "\n",
        "                generation_times.append(generation_time)\n",
        "\n",
        "                print(f\"\\n{model_name} - Candidate {i+1}: {generation_time:.2f}s\")\n",
        "                print(f\"Output length: {len(resume_text)} characters\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error generating for candidate {i+1}: {e}\")\n",
        "                generated_resumes.append({\n",
        "                    \"candidate_id\": i,\n",
        "                    \"resume\": f\"ERROR: {str(e)}\",\n",
        "                    \"generation_time_seconds\": 0,\n",
        "                    \"model\": model_name,\n",
        "                    \"error\": str(e)\n",
        "                })\n",
        "\n",
        "        del model\n",
        "        del tokenizer\n",
        "        if device == \"cuda\":\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        return {\n",
        "            \"model_name\": model_name,\n",
        "            \"model_path\": model_path,\n",
        "            \"resumes\": generated_resumes,\n",
        "            \"avg_generation_time\": sum(generation_times) / len(generation_times) if generation_times else 0,\n",
        "            \"total_time\": sum(generation_times),\n",
        "            \"status\": \"success\"\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to load {model_name}: {e}\")\n",
        "        return {\n",
        "            \"model_name\": model_name,\n",
        "            \"model_path\": model_path,\n",
        "            \"resumes\": [],\n",
        "            \"error\": str(e),\n",
        "            \"status\": \"failed\"\n",
        "        }\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c8bb22c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3c8bb22c",
        "outputId": "b78e435b-8f77-41e7-f3ff-f0dd647f2984"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preparing prompts...\n",
            "Prepared 1 prompts for testing\n",
            "------------------------------\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(\"Preparing prompts...\")\n",
        "prompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides candidate details and a target job.\n",
        "Write a professional, ATS-friendly resume tailored to the target role.\n",
        "\n",
        "First, produce a concise 2–4 bullet **Plan** that lists the sections and focus points you will include (e.g., highlight leadership, quantify achievements, include keywords from job description). Do not reveal internal chain-of-thought — keep the plan explicit and short.\n",
        "\n",
        "Then generate the resume. Use clear section headers (Summary, Experience, Education, Skills, Projects/Certs as applicable). For experience bullets, use the STAR/impact style (situation/task → action → measurable result) and include quantifiable metrics where possible. Tailor language and keywords to the target job.\n",
        "\n",
        "### Candidate details / Job target:\n",
        "{}\n",
        "\n",
        "### Additional instructions (tone, length, must-include keywords, formatting notes):\n",
        "{}\n",
        "\n",
        "### Output format:\n",
        "Plan:\n",
        "- <short bullet 1>\n",
        "- <short bullet 2>\n",
        "\n",
        "Resume:\n",
        "[Use sections: Summary, Experience (most recent first), Education, Skills, Projects/Certifications, Additional information (optional)]\n",
        "\"\"\"\n",
        "\n",
        "complex_cot = (\n",
        "    \"- Identify key skills from the candidate's past roles.\\n\"\n",
        "    \"- Match these skills to the job description keywords.\\n\"\n",
        "    \"- Prioritize experiences that show measurable achievements.\"\n",
        ")\n",
        "\n",
        "def format_prompts(df, extra_instructions=\"Tone: professional, one-page, include relevant keywords.\"):\n",
        "    candidate_details = [\n",
        "        f\"Position: {p}\\nMore info: {m}\\nLooking For: {l}\\nHighlights: {h}\\nPrimary Keyword: {k}\"\n",
        "        for p, m, l, h, k in zip(\n",
        "            df[\"Position\"],\n",
        "            df[\"Moreinfo\"],\n",
        "            df[\"Looking For\"],\n",
        "            df[\"Highlights\"],\n",
        "            df[\"Primary Keyword\"]\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    prompts = [\n",
        "        prompt_style.format(details, extra_instructions + \"\\n\" + complex_cot)\n",
        "        for details in candidate_details\n",
        "    ]\n",
        "    return prompts\n",
        "\n",
        "test_profiles = profile.head(1)\n",
        "prompts = format_prompts(test_profiles)\n",
        "print(f\"Prepared {len(prompts)} prompts for testing\")\n",
        "print(\"-\" * 30)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7276e262",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283,
          "referenced_widgets": [
            "dbfe677a66ee44888cb4d3c3dd34eac2",
            "67eb639240404c7b942e09ae91899b87",
            "e0e16424dc4041c4ba067eeacc1b1b32",
            "92f0402e4ecf4733a2517bf33388da31",
            "87a8766d62ed4d6e8693c547044efe7d",
            "8a01772a569546cd939e47fc4528e880",
            "52494f4933754dcdaf5f640772c9d11c",
            "7a19cec21e4340d5b103e62c4a5b6fcf",
            "7acdb0c2d9f54001be2fb580f37d3990",
            "6458c1e6385045b294251caa740f5e8e",
            "2aceb1439a95413192a43672c25a4f26"
          ]
        },
        "id": "7276e262",
        "outputId": "d00d0e45-6d98-4723-f567-5bdaa58716f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting model comparison benchmark...\n",
            "\n",
            "==================================================\n",
            "Loading phi-3-medium-4k-instruct...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "WARNING:transformers_modules.microsoft.Phi_hyphen_3_hyphen_medium_hyphen_4k_hyphen_instruct.b64223aaea6fbf273c0c8cd0801d5e732dce8897.modeling_phi3:`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
            "WARNING:transformers_modules.microsoft.Phi_hyphen_3_hyphen_medium_hyphen_4k_hyphen_instruct.b64223aaea6fbf273c0c8cd0801d5e732dce8897.modeling_phi3:Current `flash-attenton` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dbfe677a66ee44888cb4d3c3dd34eac2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the disk and cpu.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model phi-3-medium-4k-instruct loaded successfully\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating with phi-3-medium-4k-instruct: 100%|██████████| 1/1 [00:00<00:00,  5.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error generating for candidate 1: 'DynamicCache' object has no attribute 'seen_tokens'\n",
            "✅ phi-3-medium-4k-instruct: 0.00s avg per resume\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "all_results = {\n",
        "    \"test_config\": {\n",
        "        \"device\": device,\n",
        "        \"test_date\": datetime.now().isoformat(),\n",
        "        \"number_of_candidates\": 1,\n",
        "        \"max_new_tokens\": 1500,\n",
        "        \"total_profiles_available\": len(profile),\n",
        "        \"profiles_used\": len(test_profiles)\n",
        "    },\n",
        "    \"model_results\": []\n",
        "}\n",
        "\n",
        "print(\"Starting model comparison benchmark...\")\n",
        "\n",
        "for model_name, model_path in MODELS_TO_TEST.items():\n",
        "    result = generate_resumes_for_model(model_name, model_path, prompts, device)\n",
        "    all_results[\"model_results\"].append(result)\n",
        "\n",
        "    if result[\"status\"] == \"success\":\n",
        "        print(f\"✅ {model_name}: {result['avg_generation_time']:.2f}s avg per resume\")\n",
        "    else:\n",
        "        print(f\"❌ {model_name}: Failed - {result.get('error', 'Unknown error')}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cda314b1",
      "metadata": {
        "id": "cda314b1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0faaeab-eec1-4e57-8afa-e0978f929c81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "Benchmark completed! Results saved to: resume_generation_benchmark_20251206_224054.json\n"
          ]
        }
      ],
      "source": [
        "\n",
        "output_filename = f\"resume_generation_benchmark_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
        "\n",
        "with open(output_filename, 'w', encoding='utf-8') as f:\n",
        "    json.dump(all_results, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(\"Benchmark completed! Results saved to:\", output_filename)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0fb3bad7",
      "metadata": {
        "id": "0fb3bad7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2577645-ce7b-48a4-f4ec-8d9e32cbf691"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "PERFORMANCE SUMMARY:\n",
            "==================================================\n",
            "SUCCESSFUL MODELS:\n",
            "  phi-3-medium-4k-instruct  |   0.00s avg |   0.00s total\n",
            "\n",
            "Device used: cuda\n",
            "GPU memory allocated: 12.37 GB\n",
            "\n",
            "SAMPLE RESULTS:\n",
            "==================================================\n",
            "\n",
            "phi-3-medium-4k-instruct - First resume preview:\n",
            "ERROR: 'DynamicCache' object has no attribute 'seen_tokens'\n",
            "------------------------------\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(\"\\nPERFORMANCE SUMMARY:\")\n",
        "print(\"=\" * 50)\n",
        "successful_models = [r for r in all_results[\"model_results\"] if r[\"status\"] == \"success\"]\n",
        "failed_models = [r for r in all_results[\"model_results\"] if r[\"status\"] == \"failed\"]\n",
        "\n",
        "if successful_models:\n",
        "    print(\"SUCCESSFUL MODELS:\")\n",
        "    for result in successful_models:\n",
        "        print(f\"  {result['model_name']:25} | {result['avg_generation_time']:6.2f}s avg | {result['total_time']:6.2f}s total\")\n",
        "\n",
        "if failed_models:\n",
        "    print(\"\\nFAILED MODELS:\")\n",
        "    for result in failed_models:\n",
        "        print(f\"  {result['model_name']:25} | {result.get('error', 'Unknown error')}\")\n",
        "\n",
        "print(f\"\\nDevice used: {device}\")\n",
        "if device == \"cuda\":\n",
        "    print(f\"GPU memory allocated: {torch.cuda.max_memory_allocated() / 1024**3:.2f} GB\")\n",
        "\n",
        "print(f\"\\nSAMPLE RESULTS:\")\n",
        "print(\"=\" * 50)\n",
        "for result in successful_models:\n",
        "    if result[\"resumes\"]:\n",
        "        first_resume = result[\"resumes\"][0][\"resume\"]\n",
        "        preview = first_resume[:200] + \"...\" if len(first_resume) > 200 else first_resume\n",
        "        print(f\"\\n{result['model_name']} - First resume preview:\")\n",
        "        print(preview)\n",
        "        print(\"-\" * 30)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "dbfe677a66ee44888cb4d3c3dd34eac2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_67eb639240404c7b942e09ae91899b87",
              "IPY_MODEL_e0e16424dc4041c4ba067eeacc1b1b32",
              "IPY_MODEL_92f0402e4ecf4733a2517bf33388da31"
            ],
            "layout": "IPY_MODEL_87a8766d62ed4d6e8693c547044efe7d"
          }
        },
        "67eb639240404c7b942e09ae91899b87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8a01772a569546cd939e47fc4528e880",
            "placeholder": "​",
            "style": "IPY_MODEL_52494f4933754dcdaf5f640772c9d11c",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "e0e16424dc4041c4ba067eeacc1b1b32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7a19cec21e4340d5b103e62c4a5b6fcf",
            "max": 6,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7acdb0c2d9f54001be2fb580f37d3990",
            "value": 6
          }
        },
        "92f0402e4ecf4733a2517bf33388da31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6458c1e6385045b294251caa740f5e8e",
            "placeholder": "​",
            "style": "IPY_MODEL_2aceb1439a95413192a43672c25a4f26",
            "value": " 6/6 [00:57&lt;00:00,  7.80s/it]"
          }
        },
        "87a8766d62ed4d6e8693c547044efe7d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a01772a569546cd939e47fc4528e880": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "52494f4933754dcdaf5f640772c9d11c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7a19cec21e4340d5b103e62c4a5b6fcf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7acdb0c2d9f54001be2fb580f37d3990": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6458c1e6385045b294251caa740f5e8e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2aceb1439a95413192a43672c25a4f26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}